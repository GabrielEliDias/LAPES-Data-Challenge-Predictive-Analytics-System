{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca97628",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "### Exploratory data Analysis \n",
    "# Initial Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f672a6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"E:\\\\Programacao\\\\LAPES\\\\EDA_ML_DL_PS\\\\LAPES-Data-Challenge-Predictive-Analytics-System\\\\data\\\\bronze\\\\creditcard.csv\" \n",
    "df = pd.read_csv(path)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad41701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for nulls\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80affeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicate entries\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4cb219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see these duplicates\n",
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498e7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the dataset for duplicates\n",
    "print(f'df shape before removing duplicates: {df.shape}')\n",
    "print('removing duplicates...')\n",
    "# Noted: the 'duplicates' that pandas finds and drops are exact duplicates, the first intance is \n",
    "# the default to be kept in the dataset\n",
    "df_cleaned = df.drop_duplicates()\n",
    "print(f'removed rows: {df_cleaned.shape[0]-df_cleaned.shape[0]} ; shape after removal: {df_cleaned.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e96688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking info()\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c0fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets analyze the class column by first checking the imbalance in the Class column\n",
    "df_cleaned.value_counts(\"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bed3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting to see the fraud class imbalance in a piechart\n",
    "fraud_class = df_cleaned.Class\n",
    "fraud_class.value_counts().plot.pie(autopct=\"%.2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fe38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting to see the fraud class imbalance in a barplot\n",
    "fraud_class.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40228df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noting the percentile proportion\n",
    "fraud_class.value_counts() / fraud_class.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83318f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuing initial analysis\n",
    "df_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa738a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the minimum for amount is zero, these 0 amount transactions are sometimes used to test cards to verify validity. see: https://solidgate.com/glossary/zero-value-authorization/\n",
    "# lets analyze how many of these zero value avaliations where actually frauds or not\n",
    "df_cleaned[df_cleaned['Amount'] == 0]['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b42080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max time in seconds: 172792/3600 = 47.997 hours. Coresponds with the two days worth of transaction data\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Percentage of transactions amount')\n",
    "amount_percentage_histplot = sns.histplot(data=df_cleaned, x='Amount', binwidth=50,binrange=(0,1000),stat='percent')\n",
    "\n",
    "for patch in amount_percentage_histplot.patches:\n",
    "    height = patch.get_height()\n",
    "    if height > 0:\n",
    "        amount_percentage_histplot.annotate(f'{height:.1f}%',  # format as percentages\n",
    "            (patch.get_x() + patch.get_width() / 2, height), # setting each coordinate to be on the patch\n",
    "            ha='center', va='bottom', fontsize=8, rotation=0) # position related to the patch cordinate, right above and center\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show() # Graph discussion still needed after this as md talking about how almost 70$ of all transactions are bellow 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e329a9a5",
   "metadata": {},
   "source": [
    "### Feature Engineering for Exploration: Creating `Amount_log`\n",
    "\n",
    "During the exploratory data analysis (EDA), we observed that the `Amount` feature shows a **strong positive skew**, with most transactions being low-value and a small number of very high-value outliers.\n",
    "\n",
    "To better visualize and analyze this distribution, we applied a **logarithmic transformation**, creating a new column called `Amount_log`. This transformation helps:\n",
    "\n",
    "- Reduce the impact of extreme outliers\n",
    "- Make the distribution more normal-like\n",
    "- Allow for clearer visualization in boxplots and other graphs\n",
    "- Provide better interpretability for class comparisons (fraud vs non-fraud)\n",
    "\n",
    "---\n",
    "\n",
    "**Important Note:**  \n",
    "This feature (`Amount_log`) was created **only for EDA and visualization purposes**.  \n",
    "It will **not be included in the Silver data layer**, in accordance with good data lake practices.\n",
    "\n",
    "> All formal feature engineering steps (including this log transformation, if we choose to use it for modeling after later testing) **will be implemented inside the ML pipeline (`/src/ml_pipeline.py`)** when building our modeling dataset (Gold layer).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ce280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount varies a lot, lets look more into that and see if any of the outliers are frauds\n",
    "# df.loc[:,\"Amount\":\"Class\"].sort_values(by='Amount', ascending=False)\n",
    "# There are a couple of outliers, namely this 25000\n",
    "# However the first fraud only shows up at Amount 2125.87, so this high variance shouldnt affect the model later\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# log-scaling in order to properly see the data that would be obscured by the outliers\n",
    "df_cleaned['Amount_log'] = np.log(df_cleaned['Amount'] + 1)\n",
    "sns.boxplot(data = df_cleaned, x='Amount_log',y='Class', orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25efaf5e",
   "metadata": {},
   "source": [
    "As expected, since there is a lot less data about the fraud transactions, it is more spread out\n",
    "Noticibly it's values are generally lower, indicating most fraud transactions tend to have smaller, more common amounts. Although we can see the maximums and minimums roughly align with regular transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2c7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between features -> very usefull later in machine learning process for feature engineering and dropping\n",
    "\n",
    "corr = df_cleaned.corr()\n",
    "sns.heatmap(corr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7df262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too many features! lets see the correlation only with our target Class, for better understanding\n",
    "Top_corr = df_cleaned.corr()['Class'].drop('Class').abs().sort_values(ascending=False)\n",
    "print(Top_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb87fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the one-dimensionality of the correlation, here is a barplot showing the most and least absolutely correlated\n",
    "# features \n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(Top_corr)\n",
    "plt.title(\"Features correlated to fraud (absolute value)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aac40dc",
   "metadata": {},
   "source": [
    "WIP: here there should be a markdown talking about the graph, btw all the graphs need title and captions so those should be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e595c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What changes between non-fraudulent and fraudulent\n",
    "df_cleaned.groupby('Class').median() # median used beacause it reflects better the log-transformed Amount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b553e8",
   "metadata": {},
   "source": [
    "### File Format Decisions Across Data Layers\n",
    "\n",
    "Given that the original dataset from Kaggle was provided in CSV format, we chose to **keep the raw CSV file in the Bronze layer**, in line with data lake best practices of preserving source data in its native, raw format.\n",
    "\n",
    "Starting from the Silver layer onward, we converted the data to **Parquet format**, which offers:\n",
    "\n",
    "- Better compression\n",
    "- Faster read/write speeds\n",
    "- More efficient downstream processing for ML and dashboard tasks\n",
    "\n",
    "This design choice aligns with the Medallion Architecture principle of using **Bronze for raw input preservation** and **Parquet for optimized internal layers**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411bdc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading cleaned data into the silver layer as parquet\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent \n",
    "output_path = project_root / 'data' / 'silver' / 'creditcard_fraud_cleaned.parquet'\n",
    "\n",
    "df_cleaned.drop(\"Amount_log\", axis=1)\n",
    "df_cleaned.to_parquet(output_path, index=False,engine=\"pyarrow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eeaa37",
   "metadata": {},
   "source": [
    "### Column Naming Convention\n",
    "\n",
    "As part of our Silver layer cleaning final pipeline, we standardized all column names to snake_case.  \n",
    "This helps ensure naming consistency across all layers (Silver, Gold, and Diamond) and reduces risks of errors during downstream ML processing.\n",
    "\n",
    "---\n",
    "\n",
    "### Handling Missing Values (Nulls)\n",
    "\n",
    "Although our EDA confirmed that there are no missing values in the dataset,  \n",
    "we retained a defensive null-removal step in the Silver data cleaning pipeline.  \n",
    "This ensures that future pipeline runs remain robust even if upstream data sources change.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAPES-Data-Challenge-Predictive-Analytics-System",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
